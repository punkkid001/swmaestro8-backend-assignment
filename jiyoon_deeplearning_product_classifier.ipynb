{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝 기반 상품 카테고리 자동 분류 서버 예"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일에서 학습 데이터를 읽는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import gensim\n",
    "import requests\n",
    "\n",
    "import numpy\n",
    "from numpy import array, argmax\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "import keras\n",
    "import keras.preprocessing.text\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_text_list = []\n",
    "y_text_list = []\n",
    "enc = sys.getdefaultencoding()\n",
    "with open(\"refined_category_dataset.dat\",encoding=enc) as fin:\n",
    "    for line in fin.readlines():\n",
    "        info = json.loads(line.strip())\n",
    "        x_text_list.append((info['pid'],info['name']))\n",
    "        y_text_list.append(info['cate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(y_name_id_dict,\"y_name_id_dict.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text 형식으로 되어 있는 카테고리 명을 숫자 id 형태로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_name_id_dict = joblib.load(\"y_name_id_dict.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'가구/인테리어': 11, '반려동물': 4, '도서/문구': 8, '의류': 2, '여행/e쿠폰': 15, '식품': 6, '건강': 7, '뷰티': 0, '디지털': 10, '출산/육아': 16, '스포츠/레저': 9, '잡화': 14, '컴퓨터': 3, '자동차/공구': 1, '생필품/주방': 13, '가전': 12, '취미': 5}\n"
     ]
    }
   ],
   "source": [
    "print(y_name_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_name_set = set(y_text_list)\n",
    "#y_name_id_dict = dict(zip(y_name_set, range(len(y_name_set))))\n",
    "#print(y_name_id_dict.items())\n",
    "#y_id_name_dict = dict(zip(range(len(y_name_set)),y_name_set))\n",
    "y_list = [y_name_id_dict[x] for x in y_text_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test 분리하는 방법 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test , y_train, y_test = train_test_split(x_text_list, y_list, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 기반 text 분류에 필요한 모듈 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 파일을 만약 만들었다면, 아래와 같이 로드 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.word2vec.Word2Vec.load('word_models.model')\n",
    "word2vec.init_sims(replace=True)\n",
    "\n",
    "#model = KeyedVectors.load('ko.bin')\n",
    "\n",
    "#word2vec = model\n",
    "#word2vec.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test 데이터 새로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma = Kkma()\n",
    "\n",
    "kkma_x_train = list(map(lambda x: kkma.sentences(x[1])[0], x_train))\n",
    "kkma_x_test = list(map(lambda x: kkma.sentences(x[1].replace('/', '').replace('(', '').replace('#', '') \n",
    "                .replace(')', '').replace('[', '').replace(']', ''))[0], x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text 데이터를 word-id 형태로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_tokenizer = keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "sequence_tokenizer.fit_on_texts(kkma_x_train)\n",
    "max_features = len(sequence_tokenizer.word_index)\n",
    "\n",
    "\n",
    "def texts_to_sequences2(d_list, tokenizer, maxlen=300):\n",
    "    seq = tokenizer.texts_to_sequences(d_list)\n",
    "    print('mean:', numpy.mean([len(x) for x in seq]))\n",
    "    print('std:', numpy.std([len(x) for x in seq]))\n",
    "    print('median:', numpy.median([len(x) for x in seq]))\n",
    "    print('max:', numpy.max([len(x) for x in seq]))\n",
    "    seq = keras.preprocessing.sequence.pad_sequences(seq, maxlen=maxlen, padding='post', truncating='post')\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = texts_to_sequences2(kkma_x_train, sequence_tokenizer)\n",
    "test = texts_to_sequences2(kkma_x_test, sequence_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word의 embedding 형태의 weight 를 초기화 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = train.shape[1]\n",
    "\n",
    "input_tensor = keras.layers.Input(shape=(input_dim,), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_dim = 100\n",
    "#word_vec_dim = 200\n",
    "not_ct = 0\n",
    "weights = numpy.zeros((max_features + 1, word_vec_dim))\n",
    "for word, index in sequence_tokenizer.word_index.items():\n",
    "    if False:\n",
    "        pass\n",
    "    if word in word2vec.wv.vocab:\n",
    "        weights[index, :] = word2vec[word]\n",
    "    else:\n",
    "        not_ct+=1\n",
    "        weights[index, :] = numpy.random.uniform(-0.25, 0.25, word_vec_dim)\n",
    "# del word2vec\n",
    "# del sequence_tokenizer\n",
    "print (not_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  학습할 레이어를 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded = keras.layers.Embedding(input_dim=max_features + 1,\n",
    "                                  output_dim=word_vec_dim, input_length=input_dim,\n",
    "                                  weights=[weights],trainable=True)(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = []\n",
    "for filter_length in [3, 5]:\n",
    "    tensor = keras.layers.Convolution1D(nb_filter=50, filter_length=filter_length)(embedded)\n",
    "    tensor = keras.layers.Dropout(0.5)(tensor)\n",
    "    tensor = keras.layers.Activation('elu')(tensor)\n",
    "    tensor = keras.layers.MaxPooling1D(pool_length=input_dim - filter_length + 1)(tensor)\n",
    "    tensor = keras.layers.Flatten()(tensor)\n",
    "    tensors.append(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# embedded = keras.layers.Dropout(0.5)(embedded)\n",
    "output_tensor = keras.layers.merge(tensors, mode='concat', concat_axis=1)\n",
    "output_tensor = keras.layers.Dropout(0.5)(output_tensor) \n",
    "output_tensor = keras.layers.Dense(len(set(y_list)), activation='softmax')(output_tensor)\n",
    "\n",
    "# output = Dense(NUM_CLASSES, input_dim = hidden_dim_2, activation = \"softmax\")(pool_rnn) # See equations (6) and (7).\n",
    "\n",
    "cnn = keras.models.Model(input_tensor, output_tensor)\n",
    "cnn.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(cnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "cnn.fit(train, numpy.asarray(to_categorical(y_train)), batch_size=30, nb_epoch=20,\n",
    "        validation_data=(test, numpy.asarray(to_categorical(y_test))))\n",
    "'''\n",
    "cnn.fit(train, numpy.asarray(to_categorical(y_train)), batch_size=30, epochs=20,\n",
    "        validation_data=(test, numpy.asarray(to_categorical(y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'test'\n",
    "#mode = 'eval'\n",
    "\n",
    "eval_x_text_list = []\n",
    "if mode == 'test':\n",
    "    with open(\"soma8_test_data.dat\",encoding=enc) as fin:\n",
    "        for line in fin.readlines():\n",
    "            info = json.loads(line.strip())\n",
    "            eval_x_text_list.append((info['pid'],info['name']))\n",
    "else:\n",
    "    with open(\"soma8_eval_data.dat\",encoding=enc) as fin:\n",
    "        for line in fin.readlines():\n",
    "            info = json.loads(line.strip())\n",
    "            eval_x_text_list.append((info['pid'],info['name']))\n",
    "kkma_eval_x_text_list = list(map(lambda x: kkma.sentences(x[1].replace('/', '').replace('(', '')\n",
    "                            .replace(')', '').replace('[', '').replace(']', '').replace('#', ''))[0], eval_x_text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_x_list = texts_to_sequences2(map(lambda i : i[1],eval_x_text_list),sequence_tokenizer)\n",
    "#eval_x_text_list = texts_to_sequences2(kkma_eval_x_text_list, sequence_tokenizer)\n",
    "#eval_x_list = texts_to_sequences2(map(lambda i : i[1],eval_x_text_list),sequence_tokenizer)\n",
    "eval_x_list = texts_to_sequences2(kkma_eval_x_text_list, sequence_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_list = clf.predict(vectorizer.transform(map(lambda i : i[1],eval_x_text_list)))\n",
    "#pred = cnn.predict(eval_x_list)\n",
    "pred = cnn.predict(eval_x_list)\n",
    "pred_list = [argmax(y) for y in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='하지윤'\n",
    "nickname='punk_zzang3'\n",
    "\n",
    "param = {\n",
    "    'pred_list': \",\".join(map(lambda i : str(int(i)), pred_list)),\n",
    "    'name': name,\n",
    "    'nickname': nickname,\n",
    "    'mode': mode\n",
    "}\n",
    "d = requests.post('http://eval.buzzni.net:20001/eval', data=param)\n",
    "print(d.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
