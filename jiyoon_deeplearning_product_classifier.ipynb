{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝 기반 상품 카테고리 자동 분류 서버 예"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일에서 학습 데이터를 읽는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import gensim\n",
    "import requests\n",
    "\n",
    "import numpy\n",
    "from numpy import array, argmax\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "import keras\n",
    "import keras.preprocessing.text\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_text_list = []\n",
    "y_text_list = []\n",
    "enc = sys.getdefaultencoding()\n",
    "with open(\"refined_category_dataset.dat\",encoding=enc) as fin:\n",
    "    for line in fin.readlines():\n",
    "        info = json.loads(line.strip())\n",
    "        x_text_list.append((info['pid'],info['name']))\n",
    "        y_text_list.append(info['cate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(y_name_id_dict,\"y_name_id_dict.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text 형식으로 되어 있는 카테고리 명을 숫자 id 형태로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_name_id_dict = joblib.load(\"y_name_id_dict.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'가구/인테리어': 11, '반려동물': 4, '도서/문구': 8, '의류': 2, '여행/e쿠폰': 15, '식품': 6, '건강': 7, '뷰티': 0, '디지털': 10, '출산/육아': 16, '스포츠/레저': 9, '잡화': 14, '컴퓨터': 3, '자동차/공구': 1, '생필품/주방': 13, '가전': 12, '취미': 5}\n"
     ]
    }
   ],
   "source": [
    "print(y_name_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_name_set = set(y_text_list)\n",
    "#y_name_id_dict = dict(zip(y_name_set, range(len(y_name_set))))\n",
    "#print(y_name_id_dict.items())\n",
    "#y_id_name_dict = dict(zip(range(len(y_name_set)),y_name_set))\n",
    "y_list = [y_name_id_dict[x] for x in y_text_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test 분리하는 방법 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test , y_train, y_test = train_test_split(x_text_list, y_list, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 기반 text 분류에 필요한 모듈 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 파일을 만약 만들었다면, 아래와 같이 로드 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.word2vec.Word2Vec.load('word_models.model')\n",
    "word2vec.init_sims(replace=True)\n",
    "\n",
    "#model = KeyedVectors.load('ko.bin')\n",
    "\n",
    "#word2vec = model\n",
    "#word2vec.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test 데이터 새로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma = Kkma()\n",
    "\n",
    "kkma_x_train = list(map(lambda x: kkma.sentences(x[1])[0], x_train))\n",
    "kkma_x_test = list(map(lambda x: kkma.sentences(x[1].replace('/', '').replace('(', '') \n",
    "                .replace(')', '').replace('[', '').replace(']', ''))[0], x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text 데이터를 word-id 형태로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_tokenizer = keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "sequence_tokenizer.fit_on_texts(kkma_x_train)\n",
    "max_features = len(sequence_tokenizer.word_index)\n",
    "\n",
    "\n",
    "def texts_to_sequences2(d_list, tokenizer, maxlen=300):\n",
    "    seq = tokenizer.texts_to_sequences(d_list)\n",
    "    print('mean:', numpy.mean([len(x) for x in seq]))\n",
    "    print('std:', numpy.std([len(x) for x in seq]))\n",
    "    print('median:', numpy.median([len(x) for x in seq]))\n",
    "    print('max:', numpy.max([len(x) for x in seq]))\n",
    "    seq = keras.preprocessing.sequence.pad_sequences(seq, maxlen=maxlen, padding='post', truncating='post')\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 12.2513235294\n",
      "std: 5.73576868135\n",
      "median: 11.0\n",
      "max: 43\n",
      "mean: 10.01\n",
      "std: 5.2020827277\n",
      "median: 9.0\n",
      "max: 40\n"
     ]
    }
   ],
   "source": [
    "train = texts_to_sequences2(kkma_x_train, sequence_tokenizer)\n",
    "test = texts_to_sequences2(kkma_x_test, sequence_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word의 embedding 형태의 weight 를 초기화 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = train.shape[1]\n",
    "\n",
    "input_tensor = keras.layers.Input(shape=(input_dim,), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13914\n"
     ]
    }
   ],
   "source": [
    "word_vec_dim = 100\n",
    "#word_vec_dim = 200\n",
    "not_ct = 0\n",
    "weights = numpy.zeros((max_features + 1, word_vec_dim))\n",
    "for word, index in sequence_tokenizer.word_index.items():\n",
    "    if False:\n",
    "        pass\n",
    "    if word in word2vec.wv.vocab:\n",
    "        weights[index, :] = word2vec[word]\n",
    "    else:\n",
    "        not_ct+=1\n",
    "        weights[index, :] = numpy.random.uniform(-0.25, 0.25, word_vec_dim)\n",
    "# del word2vec\n",
    "# del sequence_tokenizer\n",
    "print (not_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  학습할 레이어를 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded = keras.layers.Embedding(input_dim=max_features + 1,\n",
    "                                  output_dim=word_vec_dim, input_length=input_dim,\n",
    "                                  weights=[weights],trainable=True)(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dzzp/.local/lib/python3.5/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(filters=50, kernel_size=3)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/dzzp/.local/lib/python3.5/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=298)`\n",
      "  \n",
      "/home/dzzp/.local/lib/python3.5/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(filters=50, kernel_size=5)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/dzzp/.local/lib/python3.5/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=296)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "tensors = []\n",
    "for filter_length in [3, 5]:\n",
    "    tensor = keras.layers.Convolution1D(nb_filter=50, filter_length=filter_length)(embedded)\n",
    "    tensor = keras.layers.Dropout(0.5)(tensor)\n",
    "    tensor = keras.layers.Activation('elu')(tensor)\n",
    "    tensor = keras.layers.MaxPooling1D(pool_length=input_dim - filter_length + 1)(tensor)\n",
    "    tensor = keras.layers.Flatten()(tensor)\n",
    "    tensors.append(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 300)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 300, 100)      1882300     input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 298, 50)       15050       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)                (None, 296, 50)       25050       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 298, 50)       0           conv1d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 296, 50)       0           conv1d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 298, 50)       0           dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 296, 50)       0           dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)   (None, 1, 50)         0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)   (None, 1, 50)         0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 50)            0           max_pooling1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 50)            0           max_pooling1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 100)           0           flatten_1[0][0]                  \n",
      "                                                                   flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 100)           0           merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 17)            1717        dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1,924,117\n",
      "Trainable params: 1,924,117\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dzzp/.local/lib/python3.5/site-packages/ipykernel_launcher.py:2: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "# embedded = keras.layers.Dropout(0.5)(embedded)\n",
    "output_tensor = keras.layers.merge(tensors, mode='concat', concat_axis=1)\n",
    "output_tensor = keras.layers.Dropout(0.5)(output_tensor) \n",
    "output_tensor = keras.layers.Dense(len(set(y_list)), activation='softmax')(output_tensor)\n",
    "\n",
    "# output = Dense(NUM_CLASSES, input_dim = hidden_dim_2, activation = \"softmax\")(pool_rnn) # See equations (6) and (7).\n",
    "\n",
    "cnn = keras.models.Model(input_tensor, output_tensor)\n",
    "cnn.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(cnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6800 samples, validate on 1700 samples\n",
      "Epoch 1/20\n",
      "6800/6800 [==============================] - 2s - loss: 2.7706 - acc: 0.1293 - val_loss: 2.6565 - val_acc: 0.3565\n",
      "Epoch 2/20\n",
      "6800/6800 [==============================] - 1s - loss: 2.4526 - acc: 0.2674 - val_loss: 2.4632 - val_acc: 0.4624\n",
      "Epoch 3/20\n",
      "6800/6800 [==============================] - 1s - loss: 2.2072 - acc: 0.3465 - val_loss: 2.2899 - val_acc: 0.5047\n",
      "Epoch 4/20\n",
      "6800/6800 [==============================] - 1s - loss: 2.0094 - acc: 0.4016 - val_loss: 2.1415 - val_acc: 0.5341\n",
      "Epoch 5/20\n",
      "6800/6800 [==============================] - 1s - loss: 1.8762 - acc: 0.4476 - val_loss: 2.0283 - val_acc: 0.5594\n",
      "Epoch 6/20\n",
      "6800/6800 [==============================] - 1s - loss: 1.7541 - acc: 0.4844 - val_loss: 1.9340 - val_acc: 0.5659\n",
      "Epoch 7/20\n",
      "6800/6800 [==============================] - 1s - loss: 1.6625 - acc: 0.5063 - val_loss: 1.8562 - val_acc: 0.5835\n",
      "Epoch 8/20\n",
      "6800/6800 [==============================] - 1s - loss: 1.6013 - acc: 0.5225 - val_loss: 1.7940 - val_acc: 0.5988\n",
      "Epoch 9/20\n",
      "6800/6800 [==============================] - 1s - loss: 1.5297 - acc: 0.5562 - val_loss: 1.7416 - val_acc: 0.6047\n",
      "Epoch 10/20\n",
      "6800/6800 [==============================] - 1s - loss: 1.4711 - acc: 0.5716 - val_loss: 1.6918 - val_acc: 0.6082\n",
      "Epoch 11/20\n",
      "6800/6800 [==============================] - 1s - loss: 1.4059 - acc: 0.5935 - val_loss: 1.6504 - val_acc: 0.6188\n",
      "Epoch 12/20\n",
      "6800/6800 [==============================] - 1s - loss: 1.3465 - acc: 0.6059 - val_loss: 1.6086 - val_acc: 0.6259\n",
      "Epoch 13/20\n",
      "6800/6800 [==============================] - 1s - loss: 1.2992 - acc: 0.6237 - val_loss: 1.5750 - val_acc: 0.6288\n",
      "Epoch 14/20\n",
      "6800/6800 [==============================] - 1s - loss: 1.2647 - acc: 0.6453 - val_loss: 1.5510 - val_acc: 0.6341\n",
      "Epoch 15/20\n",
      "6800/6800 [==============================] - 1s - loss: 1.2156 - acc: 0.6510 - val_loss: 1.5193 - val_acc: 0.6376\n",
      "Epoch 16/20\n",
      "6800/6800 [==============================] - 1s - loss: 1.1900 - acc: 0.6660 - val_loss: 1.4962 - val_acc: 0.6400\n",
      "Epoch 17/20\n",
      "6800/6800 [==============================] - 1s - loss: 1.1598 - acc: 0.6719 - val_loss: 1.4769 - val_acc: 0.6453\n",
      "Epoch 18/20\n",
      "6800/6800 [==============================] - 1s - loss: 1.1231 - acc: 0.6831 - val_loss: 1.4507 - val_acc: 0.6465\n",
      "Epoch 19/20\n",
      "6800/6800 [==============================] - 1s - loss: 1.0835 - acc: 0.7010 - val_loss: 1.4308 - val_acc: 0.6476\n",
      "Epoch 20/20\n",
      "6800/6800 [==============================] - 1s - loss: 1.0562 - acc: 0.7106 - val_loss: 1.4158 - val_acc: 0.6500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f07f8a7dcf8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "cnn.fit(train, numpy.asarray(to_categorical(y_train)), batch_size=30, nb_epoch=20,\n",
    "        validation_data=(test, numpy.asarray(to_categorical(y_test))))\n",
    "'''\n",
    "cnn.fit(train, numpy.asarray(to_categorical(y_train)), batch_size=30, epochs=20,\n",
    "        validation_data=(test, numpy.asarray(to_categorical(y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'test'\n",
    "#mode = 'eval'\n",
    "\n",
    "eval_x_text_list = []\n",
    "if mode == 'test':\n",
    "    with open(\"soma8_test_data.dat\",encoding=enc) as fin:\n",
    "        for line in fin.readlines():\n",
    "            info = json.loads(line.strip())\n",
    "            eval_x_text_list.append((info['pid'],info['name']))\n",
    "else:\n",
    "    with open(\"soma8_eval_data.dat\",encoding=enc) as fin:\n",
    "        for line in fin.readlines():\n",
    "            info = json.loads(line.strip())\n",
    "            eval_x_text_list.append((info['pid'],info['name']))\n",
    "\n",
    "kkma_eval_x_text_list = list(map(lambda x: kkma.sentences(x[1].replace('/', '').replace('(', '')\n",
    "                            .replace(')', '').replace('[', '').replace(']', ''))[0], eval_x_text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 10.0835913313\n",
      "std: 4.96387664158\n",
      "median: 9.0\n",
      "max: 39\n"
     ]
    }
   ],
   "source": [
    "#eval_x_list = texts_to_sequences2(map(lambda i : i[1],eval_x_text_list),sequence_tokenizer)\n",
    "#eval_x_text_list = texts_to_sequences2(kkma_eval_x_text_list, sequence_tokenizer)\n",
    "#eval_x_list = texts_to_sequences2(map(lambda i : i[1],eval_x_text_list),sequence_tokenizer)\n",
    "eval_x_list = texts_to_sequences2(kkma_eval_x_text_list, sequence_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_list = clf.predict(vectorizer.transform(map(lambda i : i[1],eval_x_text_list)))\n",
    "#pred = cnn.predict(eval_x_list)\n",
    "pred = cnn.predict(eval_x_list)\n",
    "pred_list = [argmax(y) for y in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'msg': 'If you pull docker image before 2017-09-27 21:30,  pull your docker image again.', 'precision': 0.6896284829721362}\n"
     ]
    }
   ],
   "source": [
    "name='하지윤'\n",
    "nickname='punk_zzang3'\n",
    "\n",
    "param = {\n",
    "    'pred_list': \",\".join(map(lambda i : str(int(i)), pred_list)),\n",
    "    'name': name,\n",
    "    'nickname': nickname,\n",
    "    'mode': mode\n",
    "}\n",
    "d = requests.post('http://eval.buzzni.net:20001/eval', data=param)\n",
    "print(d.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
